% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/Algorithms_assessment.R
\name{Scoring_system}
\alias{Scoring_system}
\alias{Scoring_system_bootstrap}
\title{The main function for algorithms scoring based on accuracy, precision, and effectiveness.}
\usage{
Scoring_system(
  Inputs,
  method = "sort-based",
  param_sort = list(decreasing = TRUE, max.score = NULL),
  param_interval = list(trim = FALSE, reward.punishment = TRUE, decreasing = TRUE,
    hundred.percent = FALSE),
  remove.negative = FALSE,
  accuracy.metrics = c("MAE", "CMAPE"),
  precision.metrics = c("BIAS", "CMRPE")
)

Scoring_system_bootstrap(
  Times = 1000,
  Inputs,
  replace = TRUE,
  method = "sort-based",
  metrics_used = 1,
  param_sort = list(decreasing = TRUE, max.score = NULL),
  param_interval = list(trim = FALSE, reward.punishment = TRUE, decreasing = TRUE,
    hundred.percent = FALSE),
  remove.negative = FALSE,
  dont_blend = FALSE,
  verbose = TRUE
)
}
\arguments{
\item{Inputs}{The list returned form function \link{Getting_Asses_results}}

\item{method}{The method selected to score algorithms: 
\itemize{
  \item \code{sort-based} (default) which is scored by the sort of accuracy and precision 
    metrics (see more in \link{Score_algorithms_sort}). 
  \item \code{sort-based2} which is scored by the sort of accuracy and precision 
    metrics (see more in \link{Score_algorithms_sort}). 
  \item \code{interval-based} which is relatively scored by the interval of accuracy and 
    precision (used by Brewin et al. (2015) and Neil et al. (2019)). 
    See more in \link{Score_algorithms_interval}).
}}

\item{param_sort}{The parameters of function \link{Score_algorithms_sort}}

\item{param_interval}{The parameters of function \link{Score_algorithms_interval}}

\item{remove.negative}{Option to replace the negative score as zero (default as \code{FALSE})}

\item{accuracy.metrics}{accuracy used metrics, default as \code{c("MAE", "CMAPE")}}

\item{precision.metrics}{precision used metrics, default as \code{c("BIAS", "CMRPE")}}

\item{Times}{Parameter of \code{Scoring_system_bootstrap}. The bootstrap time for running 
\code{Scoring_system} (default as \code{1000})}

\item{replace}{Parameter of \code{Scoring_system_bootstrap}. The sample method for bootstrap running.
Default as \code{TRUE}. See more details in \link{sample}.}

\item{metrics_used}{The metric combination used in the function. Default is \code{1}.

If \code{metrics_used = 1} then the used metrics 
  are \code{c("MAE", "CMAPE", "BIAS", "CMRPE")}

If \code{metrics_used = 2} (dont use this) then the used metrics 
  are \code{c("MAE", "CMAPE", "BIAS", "CMRPE", "RATIO")}}

\item{dont_blend}{Whether to runing the algorithm blending process. Default is \code{FALSE}.
This is useful when you just want to score the candidate algorithms.}

\item{verbose}{Show the iteration message.}
}
\value{
The result of \code{Scoring_system} are including:
\itemize{
  \item \strong{Total_score} Data.frame of final score result with algorithm as column and cluster as row.
  \item \strong{Accuracy} Data.frame of \code{Accuracy} score with algorithm as column and cluster as row.
  \item \strong{Precision} Data.frame of \code{Precision} score with algorithm as column and cluster as row.
  \item \strong{Effectiveness} Data.frame of \code{Effectiveness} score with algorithm as column and cluster as row.
  \item \strong{Accuracy_list} List including data.frames of used \code{Accuracy} metrics.
  \item \strong{Precision_list} List including data.frames of used \code{Precision} metrics.
  \item \strong{Total_score.melt} Melted data.frame of \strong{Total_score} for plotting.
  \item \strong{Opt_algorithm} The optimal algorithm names for each cluster.
  \item \strong{Inputs} Inputs of this function.
}

The result of \code{Scoring_system_bootstrap} are including:
\itemize{
  \item \strong{Times} The times of bootstrap running.
  \item \strong{Score_all_clusters} The total score for algorithms across all clusters.
  \item \strong{Score_list} All times of bootstrapping results are recorded in it.
  \item \strong{Score_list_melt} Melted \code{Score_list}.
  \item \strong{Opt_algorithm_list} The optimal algorithm for every runing.
  \item \strong{Opt_algorithm} The optimal algorithm defined by mode of \code{Opt_algorithm_list}
    for each cluster.
  \item \strong{Remove_algorithm} The algorithms to be removed when blending.
  \item \strong{plot_col} The col plot of \code{Score_list_melt}.
  \item \strong{plot_scatter} The scatter plot of measured and predicted Chla concentration
    colored by clusters.
  \item \strong{plot_scatter_opt} The scatter plot of measured and predicted Chla concentration
    colored by clusters for optimized algorithms.
  \item \strong{Blend_result} The results from the inherent function \code{Chla_algorithms_blend}.
  \item \strong{dt_Chla} Data.frame with combination of candidate algortihms and blended results.
  \item \strong{Chla_blend} The blended Chla concentration by score results.
  \item \strong{Results_of_scoring_system} A list including all results of \link{Scoring_system} function.
  \item \strong{metric_results} A result of \link{Assessment_via_cluster} which includes the Chla blend results.
}
}
\description{
The main function for algorithms scoring based on accuracy, precision, and effectiveness.
}
\details{
The \code{Accuracy} and \code{Precision} is newly defined in \code{FCMm} package (referred by 
  Hooker et al. (2005)):
\itemize{
  \item \code{Accuracy} is the estimation of how close the result of the experiment is to the 
    true value.
  \item \code{Precision} is the estimation of how excatly the result is determined independently 
    of any true value.
}
In other words, \code{Accuracy} is telling a story truthfully and precision is how similarly
  the story is represented over and over again.
Here we use AE, a vector for each sample, for instance:
\itemize{
  \item \code{Accuracy} is the aggregation (no matter mean or median, in fuzzy calculation process), 
    we use mean to some extent. 
  \item \code{Precision} is actually the \strong{stability} of AE (reproducebility) which means the error
    produced by the algorithm is under certain control.
}
Finally, the function will multiply the total score (\code{Accuracy} + \code{Precision}) by the 
  effectiveness (i.e., Valid_percent returned by \link{Assessment_via_cluster}).
}
\note{
\code{Scoring_system_bootstrap} is the bootstrap mode of \code{Scoring_system} which is useful when
    the outcome is unstable for large number of samples. The default boostrap time in \code{Scoring_system_bootstrap}
    is set as \code{1000} and the result of it is the list of several aggregated data.frames and standard deviations.
}
\examples{
library(FCMm) 
library(ggplot2) 
library(magrittr)
library(stringr)
data("Nechad2015")
x <- Nechad2015[,3:11]
wv <- gsub("X","",names(x)) \%>\% as.numeric
set.seed(1234)
w <- sample.int(nrow(x))
x <- x[w, ]
names(x) <- wv
nb = 4 # Obtained from the vignette "Cluster a new dataset by FCMm"
set.seed(1234)
FD <- FuzzifierDetermination(x, wv, do.stand=TRUE)
result <- FCM.new(FD, nb, fast.mode = TRUE)
p.spec <- plot_spec(result, show.stand=TRUE)
print(p.spec$p.cluster.spec)
Chla <- Nechad2015$X.Chl_a..ug.L.[w]
Chla[Chla >= 999] <- NA
dt_Chla <- run_all_Chla_algorithms(x) \%>\% as.data.frame
dt_Chla <- data.frame(Chla_true = Chla,
BR_Gil10 = dt_Chla$BR_Gil10, 
OC4_OLCI = dt_Chla$OC4_OLCI, 
OCI_Hu12 = dt_Chla$OCI_Hu12, 
NDCI_Mi12= dt_Chla$NDCI_Mi12) \%>\% round(3)
w = which(!is.na(dt_Chla$Chla_true))
dt_Chla = dt_Chla[w,]
memb = result$res.FCM$u[w,] \%>\% round(4)
cluster =  result$res.FCM$cluster[w]
Asses_results <- Getting_Asses_results(sample.size=length(cluster), 
pred = dt_Chla[,-1], meas = data.frame(dt_Chla[,1]), memb = memb, 
cluster = cluster)
Score = Scoring_system(Asses_results)
# show the total score table
knitr::kable(round(Score$Total_score, 2))

# Examples of `Scoring_system_bootstrap`

set.seed(1234)
Score_boo <- Scoring_system_bootstrap(Times = 3, Asses_results) 
# try to set large `Times` when using your own data

# Show the bar plot of scores
Score_boo$plot_col

# Show the scatter plot of measure-estimation pairs
Score_boo$plot_scatter

# Show error metrics
knitr::kable(round(Score_boo$metric_results$MAE, 2), caption = "MAE")
knitr::kable(round(Score_boo$metric_results$CMAPE, 2), caption = "CAPE")
knitr::kable(round(Score_boo$metric_results$BIAS, 2), caption = "BIAS")
knitr::kable(round(Score_boo$metric_results$CMRPE, 2), caption = "CRPE")

# you would see the blending estimations outperform than other candidates

}
\references{
\itemize{
  \item Hooker S B. Second SeaWiFS HPLC analysis round-robin experiment (SeaHARRE-2)[M]. 
    National Aeronautics and Space Administration, Goddard Space Flight Center, 2005. 
  \item Seegers B N, Stumpf R P, Schaeffer B A, et al. Performance metrics for the assessment 
    of satellite data products: an ocean color case study[J]. Optics express, 2018, 26(6): 7404-7422.
  \item Neil C, Spyrakos E, Hunter P D, et al. A global approach for chlorophyll-a retrieval 
    across optically complex inland waters based on optical water types[J]. Remote Sensing of 
    Environment, 2019, 229: 159-178.
  \item Brewin R J W, Sathyendranath S, MÃ¼ller D, et al. The Ocean Colour Climate Change 
    Initiative: III. A round-robin comparison on in-water bio-optical algorithms[J]. Remote Sensing of 
    Environment, 2015, 162: 271-294.
  \item Moore T S, Dowell M D, Bradt S, et al. An optical water type framework for selecting and 
    blending retrievals from bio-optical algorithms in lakes and coastal waters[J]. Remote sensing of 
    environment, 2014, 143: 97-111.
}
}
\seealso{
Other Algorithm assessment: 
\code{\link{Assessment_via_cluster}()},
\code{\link{Getting_Asses_results}()},
\code{\link{Sampling_via_cluster}()},
\code{\link{Score_algorithms_interval}()},
\code{\link{Score_algorithms_sort}()}
}
\concept{Algorithm assessment}
